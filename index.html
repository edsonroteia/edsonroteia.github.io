<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="author" content="Edson Araujo">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Edson Araujo</title>
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" href="stylesheet.css">
  </head>

  <body>
    <div style="max-width:800px; margin:0 auto;">
      <!-- Header & Profile -->
      <section style="display:flex; align-items:flex-start;">
        <div style="flex:0 0 60%; padding:2.5%; box-sizing: border-box;">
          <h1 style="margin-top:0; text-align:center;">Edson Araujo</h1>
          <p>I'm a PhD Student at <a href="https://www.goethe-university-frankfurt.de/en">Goethe University Frankfurt</a>, working with <a href="https://hildekuehne.github.io/">Prof. Hilde Kuehne</a>. Our work is part of the  <a href="https://sightandsound.csail.mit.edu/"> MIT-IBM Watson AI Sight and Sound Project</a>, through which we work with several researchers on multi-modal learning.</p>
          <p>I did my Master's in Computer Science at <a href="https://ufmg.br/">UFMG</a> under the supervision of <a href="https://homepages.dcc.ufmg.br/~erickson/">Prof. Erickson Nascimento</a>, period in which I was able to collaborate in different research topics such as video summarization and image descriptors.</p>
          <p style="text-align:center; margin-bottom:0;">
            <a href="data/Edson_CV_May2025.pdf">CV</a> &nbsp;/&nbsp;
            <a href="https://scholar.google.com/citations?hl=en&user=tHOHQ7IAAAAJ">Scholar</a> &nbsp;/&nbsp;
            <a href="https://twitter.com/edsonroteia">Twitter</a> &nbsp;/&nbsp;
            <a href="https://bsky.app/profile/edsonroteia.bsky.social">Bluesky</a> &nbsp;/&nbsp;
            <a href="https://github.com/edsonroteia/">Github</a>
          </p>
          <p style="text-align:center; margin-bottom:0;">
            Email: <span style="font-family: monospace; font-size: small;">[last_name]</span> at uni-frankfurt.de
          </p>
        </div>
        <div style="flex:0 0 40%; padding:2.5%; box-sizing: border-box; display: flex; align-items: center; justify-content: center;">
          <img src="images/profile_pic_bright.jpeg" alt="profile photo" style="width:100%; max-width:300px; border-radius:50%;">
        </div>
      </section>

      <!-- News -->
      <section style="background:#f8f8f8; border:1px solid #e0e0e0; border-radius:8px; padding:20px; margin:20px 0;">
        <h2>ðŸ”¥ News</h2>
        <p><strong>05.2025</strong>&ensp; CAV-MAE Sync is also going to be presented at the <a href="https://www.latinxinai.org/cvpr-2025">LatinX, <a href="https://sites.google.com/view/mmfm3rdworkshop/organizers?authuser=0">MMFM</a> and <a href="https://sightsound.org/">Sight and Sound</a> Workshops</a> at CVPR 2025!</p>
        <p><strong>02.2025</strong>&ensp; CAV-MAE Sync was accepted to CVPR 2025 as a poster presentation.</p>
        <p><strong>10.2023</strong>&ensp; I joined a PhD Program under the supervision of <a href="https://hildekuehne.github.io/">Prof. Hilde Kuehne</a></p>
        <p><strong>05.2023</strong>&ensp; I defended my Master Thesis on "An Audiovisual Approach for Video Summarization Using Psychoacoustic Features"</p>
      </section>

      <!-- Research -->
      <section>
        <h2>Research</h2>
        <p>I'm interested in multimodal learning, self-supervised methods and audiovisual representation learning. Some papers are <span class="highlight">highlighted</span>.</p>
      </section>

      <!-- Publications -->
      <section>
        <h2>Selected Publications</h2>
        <table style="width:100%; table-layout:fixed; border:0; border-spacing:0; margin:0;">
          <colgroup>
            <col style="width:25%;">
            <col style="width:75%;">
          </colgroup>
          <tbody>
            <tr>
              <td style="padding:10px; vertical-align:middle; background-color: #ffffd0;">
                <img src="images/cavmaesync-removebg-preview.png" alt="CAV-MAE Placeholder" style="width:100%; height:auto; display:block;">
              </td>
              <td style="padding:20px; vertical-align:middle; background-color: #ffffd0;">
                <a href="#">
                  <span class="papertitle">CAV-MAE Sync: Improving Contrastive Audio-Visual Mask Autoencoders via Fine-Grained Alignment</span>
                </a><br>
                <strong>Edson Araujo</strong>, Andrew Rouditchenko, Yuan Gong, Saurabhchand Bhati, Samuel Thomas, Brian Kingsbury, Leonid Karlinsky, Rogerio Feris, James R. Glass, Hilde Kuehne<br>
                <em>CVPR, 2025</em><br>
                <a href="https://edsonroteia.github.io/cav-mae-sync/">Project Page</a> / <a href="https://github.com/edsonroteia/cav-mae-sync">Code</a> / <a href="https://arxiv.org/abs/2505.01237">arXiv</a>
                <p>We improved audio-visual learning by treating audio as a temporal sequence aligned with video frames instead of global representations, and by separating competing objectives with dedicated tokens. Our approach outperforms more complex architectures on retrieval, classification, and localization tasks across multiple datasets.</p>
              </td>
            </tr>
            <tr>
              <td style="padding:10px; vertical-align:middle;">
                <img src="images/vdan+.png" alt="Paper Placeholder" style="width:100%; height:auto; display:block;">
              </td>
              <td style="padding:20px; vertical-align:middle;">
                <a href="#">
                  <span class="papertitle">Text-Driven Video Acceleration: A Weakly-Supervised Reinforcement Learning Method</span>
                </a><br>
                Washington Ramos, Michel Silva, <strong>Edson Araujo</strong>, Victor Moura, Keller Oliveira, Leandro Soriano Marcolino<br>
                <em>TPAMI, 2023</em><br>
                <a href="https://www.verlab.dcc.ufmg.br/semantic-hyperlapse/tpami2023">Project Page</a> / <a href="https://github.com/verlab/TextDrivenVideoAcceleration_TPAMI_2022">Code</a> / <a href="https://arxiv.org/abs/2203.15778">arXiv</a>
                <p>This paper introduces a novel weakly-supervised reinforcement learning method to accelerate instructional videos using text, addressing limitations of current summarization techniques. The proposed approach uses a new joint reward function and the Extended Visually-guided Document Attention Network (VDAN+) to effectively control video length while achieving state-of-the-art performance.</p>
              </td>
            </tr>
            <tr>
              <td style="padding:10px; vertical-align:middle;">
                <img src="images/vdan.png" alt="Paper Placeholder CVPR 2020" style="width:100%; height:auto; display:block;">
              </td>
              <td style="padding:20px; vertical-align:middle;">
                <a href="https://www.verlab.dcc.ufmg.br/semantic-hyperlapse/cvpr2020">
                  <span class="papertitle">Straight to the Point: Fast-Forwarding Videos via Reinforcement Learning Using Textual Data</span>
                </a><br>
                Washington Ramos, Michel Silva, <strong>Edson Araujo</strong>, Leandro Soriano Marcolino, Erickson Nascimento<br>
                <em>CVPR, 2020</em><br>
                <a href="https://www.verlab.dcc.ufmg.br/semantic-hyperlapse/cvpr2020">Project Page</a> / <a href="https://github.com/verlab/StraightToThePoint_CVPR_2020">Code</a> / <a href="http://arxiv.org/abs/2003.14229">arXiv</a>
                <p>This paper presents a novel reinforcement learning methodology to accelerate instructional videos by adaptively selecting and removing irrelevant frames, addressing visual gaps in traditional summarization. The approach utilizes a textually and visually oriented agent with a new Visually-guided Document Attention Network (VDAN) for creating shorter, coherent videos and achieves state-of-the-art F1 score and segment coverage.</p>
              </td>
            </tr>
            <tr>
              <td style="padding:10px; vertical-align:middle;">
                <img src="images/wacv2020.png" alt="WACV 2020 Paper" style="width:100%; height:auto; display:block;">
              </td>
              <td style="padding:20px; vertical-align:middle;">
                <a href="https://www.verlab.dcc.ufmg.br/semantic-hyperlapse/">
                  <span class="papertitle">Personalizing Fast-Forward Videos Based on Visual and Textual Features from Social Network</span>
                </a><br>
                Washington Ramos, Michel Silva, <strong>Edson Araujo</strong>, Alan Neves, Erickson Nascimento<br>
                <em>WACV, 2020</em><br>
                <a href="https://www.verlab.dcc.ufmg.br/semantic-hyperlapse/">Project Page</a> / <a href="https://arxiv.org/abs/1912.12655">arXiv</a>
                <p>This paper introduces an approach for automatically creating personalized fast-forward videos from First-Person Videos (FPVs) by leveraging text-centric data from a user's social networks to infer their topics of interest. The method assigns scores to input frames based on user preferences and significantly outperforms competitors in F1 score, with its effectiveness demonstrated through extensive experiments and a user study.</p>
              </td>
            </tr>
            <tr>
              <td style="padding:10px; vertical-align:middle;">
                <img src="images/iros.jpeg" alt="IROS 2019 Paper" style="width:100%; height:auto; display:block;">
              </td>
              <td style="padding:20px; vertical-align:middle;">
                <a href="https://ieeexplore.ieee.org/document/8967520">
                  <span class="papertitle">On Modeling the Effects of Auditory Annoyance on Driving Style and Passenger Comfort</span>
                </a><br>
                <strong>Edson Araujo</strong>, Michal Gregor, Isabella Huang, Erickson R. Nascimento, Ruzena Bajcsy<br>
                <em>IROS, 2019</em><br>
                <a href="https://ieeexplore.ieee.org/document/8967520">Paper</a>
                <p>This study investigates the impact of acoustic annoyance on drivers in real-world scenarios, revealing significant differences in driving styles and presenting an online classifier that detects driver annoyance from inertial measurements with 77% accuracy. While empirically confirming a passenger dynamics model using pressure mat data, the study couldn't establish a change in self-reported passenger comfort due to acoustically induced driving styles not being sufficiently distinct.</p>
              </td>
            </tr>
          </tbody>
        </table>
      </section>
      <section style="text-align:right; margin-top:30px; padding-bottom:0px; font-size:smaller;">
        <p style="margin:0; font-style:italic; font-size: 0.9em;">Design and source code from <a href="https://jonbarron.info/" style="font-size:1em;">Jon Barron's website</a></p>
      </section>
    </div>
  </body>
</html>
