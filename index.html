<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="author" content="Edson Araujo">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Edson Araujo</title>
    <meta name="description" content="Edson Araujo - PhD Student at University of Tübingen. Research in audio-visual reasoning, multimodal large language models, and test-time adaptation.">
    <link rel="stylesheet" href="stylesheet.css">
  </head>

  <body>
    <a href="#main-content" class="skip-link">Skip to main content</a>
    <main id="main-content" class="main-container">
      <!-- Header & Profile -->
      <section class="profile-section">
        <div class="profile-content">
          <h1>Edson Araujo</h1>
          <p>I'm a PhD Student at <a href="https://uni-tuebingen.de/">University of Tübingen</a>, working with <a href="https://hildekuehne.github.io/">Prof. Hilde Kuehne</a>. Our work is part of the <a href="https://sightandsound.csail.mit.edu/">MIT-IBM Watson AI Sight and Sound Project</a>, where I focus on audio-visual reasoning, multimodal large language models, and test-time adaptation.</p>
          <p>I did my Master's in Computer Science at <a href="https://ufmg.br/">UFMG</a> under the supervision of <a href="https://homepages.dcc.ufmg.br/~erickson/">Prof. Erickson Nascimento</a>, period in which I was able to collaborate in different research topics such as video summarization and image descriptors.</p>
          <nav class="header-links" style="margin-bottom:0;">
            <a href="data/Edson_CV.pdf">CV</a>
            <a href="https://scholar.google.com/citations?hl=en&user=tHOHQ7IAAAAJ">Scholar</a>
            <a href="https://twitter.com/edsonroteia">Twitter</a>
            <a href="https://bsky.app/profile/edsonroteia.bsky.social">Bluesky</a>
            <a href="https://github.com/edsonroteia/">GitHub</a>
          </nav>
          <p style="text-align:center; margin-bottom:0;">
            Email: <span class="email-hint">[first_name]@[last_name].info</span>
          </p>
        </div>
        <div class="profile-image">
          <img src="images/profile_pic_bright.jpeg" alt="Portrait photo of Edson Araujo">
        </div>
      </section>

      <!-- News -->
      <section class="news-section">
        <h2>News</h2>
        <div id="news-items">
          <p class="news-item"><span class="news-date">12.2025</span> We are organizing the fifth edition of the <a href="https://mmfm-workshop.github.io/">"What is Next in Multimodal Foundation Models?"</a> Workshop (CVPR 2026)</p>
          <p class="news-item"><span class="news-date">08.2025</span> Omni-R1 was accepted to <a href="https://2025.ieeeasru.org/">ASRU 2025</a>! <em>(shortlisted for Best Student Paper!)</em></p>
          <p class="news-item"><span class="news-date">05.2025</span> Omni-R1, our latest work from the MIT-IBM Watson AI Sight and Sound Project, is out <a href="https://arxiv.org/abs/2505.09439v1">on ArXiv</a>!</p>
          <p class="news-item"><span class="news-date">05.2025</span> CAV-MAE Sync is also going to be presented at the <a href="https://www.latinxinai.org/cvpr-2025">LatinX</a>, <a href="https://sites.google.com/view/mmfm3rdworkshop/organizers?authuser=0">MMFM</a> and <a href="https://sightsound.org/">Sight and Sound</a> Workshops at CVPR 2025!</p>
          <p class="news-item"><span class="news-date">02.2025</span> CAV-MAE Sync was accepted to CVPR 2025 as a poster presentation. Paper is <a href="https://arxiv.org/abs/2505.01237">on ArXiv</a>.</p>
        </div>
        <button id="news-toggle" type="button" style="margin-top: 12px; cursor: pointer; font-size: 13px; color: #1772d0; background: none; border: none; padding: 0; font-family: inherit;" aria-expanded="false" aria-controls="more-news">Show more</button>
        <div id="more-news" style="display: none;">
          <p class="news-item"><span class="news-date">10.2023</span> I joined my PhD Program under the supervision of <a href="https://hildekuehne.github.io/">Prof. Hilde Kuehne</a> to work on multimodal learning.</p>
          <p class="news-item"><span class="news-date">05.2023</span> I defended my Master Thesis on "An Audiovisual Approach for Video Summarization Using Psychoacoustic Features"</p>
        </div>
      </section>
      <script>
        document.getElementById('news-toggle').addEventListener('click', function() {
          var moreNews = document.getElementById('more-news');
          var isExpanded = this.getAttribute('aria-expanded') === 'true';

          if (isExpanded) {
            moreNews.style.display = 'none';
            this.setAttribute('aria-expanded', 'false');
            this.textContent = 'Show more';
          } else {
            moreNews.style.display = 'block';
            this.setAttribute('aria-expanded', 'true');
            this.textContent = 'Show less';
          }
        });
      </script>

      <!-- Research -->
      <section>
        <h2>Research</h2>
        <p>I'm interested in audio-visual reasoning, multimodal large language models, self-supervised learning, and test-time adaptation. Some papers are <span class="highlight">highlighted</span>.</p>
      </section>

      <!-- Publications -->
      <section>
        <h2>Selected Publications</h2>
        <table class="pub-table">
          <colgroup>
            <col style="width:25%;">
            <col style="width:75%;">
          </colgroup>
          <tbody>
            <tr class="highlight-row">
              <td style="padding:12px; vertical-align:middle;">
                <img src="images/avrt.png" alt="AVRT architecture diagram showing audio-visual reasoning transfer from single-modality teachers" style="width:100%; height:auto; display:block;">
              </td>
              <td style="padding:16px 20px; vertical-align:middle;">
                <a href="https://edsonroteia.github.io/data/avrt.pdf">
                  <span class="papertitle">AVRT: Audio-Visual Reasoning Transfer through Single-Modality Teachers</span>
                </a>
                <div class="authors"><strong>Edson Araujo</strong>, Saurabhchand Bhati, M. Jehanzeb Mirza, Brian Kingsbury, Samuel Thomas, Rogerio Feris, James R. Glass, Hilde Kuehne</div>
                <div class="venue">Under review</div>
                <div class="paper-links"><a href="https://edsonroteia.github.io/data/avrt.pdf">pre-print</a></div>
                <p class="paper-desc">Generates high-quality audio-visual reasoning traces from single-modality teachers. Achieves superior performance on OmniBench, DailyOmni, and MMAR benchmarks.</p>
              </td>
            </tr>
                        <tr class="highlight-row">
              <td style="padding:12px; vertical-align:middle;">
                <img src="images/ttavid.png" alt="TTA-Vid framework diagram for test-time adaptation on instructional videos" style="width:100%; height:auto; display:block;" loading="lazy">
              </td>
              <td style="padding:16px 20px; vertical-align:middle;">
                <a href="https://edsonroteia.github.io/data/ttavid.pdf">
                  <span class="papertitle">TTA-Vid: Test-Time Adaptation for Long Instructional Videos</span>
                </a>
                <div class="authors">Soumya Shamarao Jahagirdar*, <strong>Edson Araujo</strong>*, Anna Kukleva, M. Jehanzeb Mirza, Saurabhchand Bhati, Samuel Thomas, Brian Kingsbury, Rogerio Feris, James R. Glass, Hilde Kuehne</div>
                <div class="venue">Under review</div>
                <div class="paper-links"><a href="https://edsonroteia.github.io/data/ttavid.pdf">pre-print</a></div>
                <p class="paper-desc">Adapts video-language models at test time using step-by-step frame reasoning and multi-armed bandit frame selection. No labels required.</p>
              </td>
            </tr>
                        <tr class="highlight-row">
              <td style="padding:12px; vertical-align:middle;">
                <img src="images/Omni-R1.png" alt="Omni-R1 model architecture for audio LLM fine-tuning" style="width:100%; height:auto; display:block;" loading="lazy">
              </td>
              <td style="padding:16px 20px; vertical-align:middle;">
                <a href="https://arxiv.org/abs/2505.09439v1">
                  <span class="papertitle">Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?</span>
                </a>
                <div class="authors">Andrew Rouditchenko, Saurabhchand Bhati, <strong>Edson Araujo</strong>, Samuel Thomas, Hilde Kuehne, Rogerio Feris, James Glass</div>
                <div class="venue">ASRU 2025</div>
                <div class="paper-links"><a href="https://arxiv.org/abs/2505.09439v1">arXiv</a></div>
                <p class="paper-desc">Fine-tunes Qwen2.5-Omni using GRPO, achieving SOTA on MMAU. Surprisingly, text-only fine-tuning also improves audio performance.</p>
              </td>
            </tr>
                        <tr class="highlight-row">
              <td style="padding:12px; vertical-align:middle;">
                <img src="images/cavmaesync-removebg-preview.png" alt="CAV-MAE Sync architecture showing fine-grained audio-visual alignment" style="width:100%; height:auto; display:block;" loading="lazy">
              </td>
              <td style="padding:16px 20px; vertical-align:middle;">
                <a href="https://edsonroteia.github.io/cav-mae-sync/">
                  <span class="papertitle">CAV-MAE Sync: Improving Contrastive Audio-Visual Mask Autoencoders via Fine-Grained Alignment</span>
                </a>
                <div class="authors"><strong>Edson Araujo</strong>, Andrew Rouditchenko, Yuan Gong, Saurabhchand Bhati, Samuel Thomas, Brian Kingsbury, Leonid Karlinsky, Rogerio Feris, James R. Glass, Hilde Kuehne</div>
                <div class="venue venue-top">CVPR 2025</div>
                <div class="paper-links"><a href="https://edsonroteia.github.io/cav-mae-sync/">Project</a> · <a href="https://github.com/edsonroteia/cav-mae-sync">Code</a> · <a href="https://arxiv.org/abs/2505.01237">arXiv</a></div>
                <p class="paper-desc">Fine-grained audio-visual alignment using temporal sequences instead of global representations. Outperforms complex architectures on retrieval, classification, and localization.</p>
              </td>
            </tr>
            <tr>
              <td style="padding:12px; vertical-align:middle;">
                <img src="images/vdan+.png" alt="VDAN+ architecture for text-driven video acceleration using reinforcement learning" style="width:100%; height:auto; display:block;" loading="lazy">
              </td>
              <td style="padding:16px 20px; vertical-align:middle;">
                <a href="https://www.verlab.dcc.ufmg.br/semantic-hyperlapse/tpami2023">
                  <span class="papertitle">Text-Driven Video Acceleration: A Weakly-Supervised Reinforcement Learning Method</span>
                </a>
                <div class="authors">Washington Ramos, Michel Silva, <strong>Edson Araujo</strong>, Victor Moura, Keller Oliveira, Leandro Soriano Marcolino</div>
                <div class="venue venue-top">TPAMI 2023</div>
                <div class="paper-links"><a href="https://www.verlab.dcc.ufmg.br/semantic-hyperlapse/tpami2023">Project</a> · <a href="https://github.com/verlab/TextDrivenVideoAcceleration_TPAMI_2022">Code</a> · <a href="https://arxiv.org/abs/2203.15778">arXiv</a></div>
                <p class="paper-desc">Weakly-supervised RL method for text-driven video acceleration using VDAN+ architecture.</p>
              </td>
            </tr>
            <tr>
              <td style="padding:12px; vertical-align:middle;">
                <img src="images/vdan.png" alt="VDAN network diagram for video fast-forwarding via reinforcement learning" style="width:100%; height:auto; display:block;" loading="lazy">
              </td>
              <td style="padding:16px 20px; vertical-align:middle;">
                <a href="https://www.verlab.dcc.ufmg.br/semantic-hyperlapse/cvpr2020">
                  <span class="papertitle">Straight to the Point: Fast-Forwarding Videos via Reinforcement Learning Using Textual Data</span>
                </a>
                <div class="authors">Washington Ramos, Michel Silva, <strong>Edson Araujo</strong>, Leandro Soriano Marcolino, Erickson Nascimento</div>
                <div class="venue venue-top">CVPR 2020</div>
                <div class="paper-links"><a href="https://www.verlab.dcc.ufmg.br/semantic-hyperlapse/cvpr2020">Project</a> · <a href="https://github.com/verlab/StraightToThePoint_CVPR_2020">Code</a> · <a href="http://arxiv.org/abs/2003.14229">arXiv</a></div>
                <p class="paper-desc">RL-based video acceleration using textual guidance and the VDAN architecture. SOTA on F1 and segment coverage.</p>
              </td>
            </tr>
            <tr>
              <td style="padding:12px; vertical-align:middle;">
                <img src="images/wacv2020.png" alt="System diagram for personalized video fast-forwarding using social network data" style="width:100%; height:auto; display:block;" loading="lazy">
              </td>
              <td style="padding:16px 20px; vertical-align:middle;">
                <a href="https://www.verlab.dcc.ufmg.br/semantic-hyperlapse/">
                  <span class="papertitle">Personalizing Fast-Forward Videos Based on Visual and Textual Features from Social Network</span>
                </a>
                <div class="authors">Washington Ramos, Michel Silva, <strong>Edson Araujo</strong>, Alan Neves, Erickson Nascimento</div>
                <div class="venue">WACV 2020</div>
                <div class="paper-links"><a href="https://www.verlab.dcc.ufmg.br/semantic-hyperlapse/">Project</a> · <a href="https://arxiv.org/abs/1912.12655">arXiv</a></div>
                <p class="paper-desc">Personalized FPV fast-forwarding using social network data to infer user interests.</p>
              </td>
            </tr>
            <tr>
              <td style="padding:12px; vertical-align:middle;">
                <img src="images/iros.jpeg" alt="Experimental setup showing driving simulation with auditory annoyance measurement" style="width:100%; height:auto; display:block;" loading="lazy">
              </td>
              <td style="padding:16px 20px; vertical-align:middle;">
                <a href="https://ieeexplore.ieee.org/document/8967520">
                  <span class="papertitle">On Modeling the Effects of Auditory Annoyance on Driving Style and Passenger Comfort</span>
                </a>
                <div class="authors"><strong>Edson Araujo</strong>, Michal Gregor, Isabella Huang, Erickson R. Nascimento, Ruzena Bajcsy</div>
                <div class="venue">IROS 2019</div>
                <div class="paper-links"><a href="https://ieeexplore.ieee.org/document/8967520">Paper</a></div>
                <p class="paper-desc">Detects driver annoyance from inertial measurements with 77% accuracy. Studies acoustic impact on driving style.</p>
              </td>
            </tr>
          </tbody>
        </table>
      </section>
      <footer class="footer">
        <p>Design inspired by <a href="https://jonbarron.info/">Jon Barron</a></p>
      </footer>
    </main>
  </body>
</html>
